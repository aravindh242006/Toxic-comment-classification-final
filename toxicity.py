# -*- coding: utf-8 -*-
"""TOXICITY.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a9DiqnLbvUckoeQiSYx3VugPAPhhH0zW
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

import matplotlib.pyplot as plt

from google.colab import files
uploaded = files.upload()

toxic=pd.read_csv(r'train.csv.zip')

df=pd.DataFrame(toxic)

df.info()

plt.figure(figsize=(16,12))


plt.subplot(2, 2, 1)
plt.hist(df["comment_length"], bins=40, color="skyblue")
plt.title("Distribution of Comment Lengths")
plt.xlabel("Length of Comment")
plt.ylabel("Frequency")

plt.subplot(2, 2, 2)
df[category_cols].sum().plot(kind="bar", color="orange")
plt.title("Count of Toxic Categories")
plt.ylabel("Number of Toxic Labels")

plt.subplot(2, 2, 3)
df["overall_toxic"].value_counts().plot(
    kind="pie",
    autopct='%1.1f%%',
    labels=["Non-Toxic", "Toxic"],
    colors=["green", "red"],
    startangle=90
)
plt.title("Overall Toxic vs Non-Toxic")
plt.ylabel("")


plt.subplot(2, 2, 4)
plt.scatter(df["comment_length"], df["overall_toxic"], alpha=0.3)
plt.title("Scatter: Comment Length vs Toxicity")
plt.xlabel("Comment Length")
plt.ylabel("Toxicity (0 or 1)")

plt.tight_layout()
plt.show()

vect=CountVectorizer()

total_toxicity=['toxic','severe_toxic','obscene','threat','insult','identity_hate']

df['toxic']=df[total_toxicity].max(axis=1)

print(df[['comment_text','toxic']].tail())

print(df['toxic'].value_counts())



vect=CountVectorizer(max_features=10000,stop_words='english',min_df=2)

x=df['comment_text']
y=df['toxic']
vect_x=vect.fit_transform(x),

x_train,x_test,y_train,y_test=train_test_split(vect_x[0],y,test_size=0.2,random_state=42)

model=MultinomialNB()

model.fit(x_train,y_train)

model.predict(x_test)

print(accuracy_score(y_test,model.predict(x_test)))

print(df[['comment_text','toxic']].head(20))

cm= confusion_matrix(y_test,model.predict(x_test))
cm

sns.heatmap(cm,annot=True)

def predict_toxic(comment_text):
    """
    Predict whether a comment is toxic or not.
    Output: 0 = Positive / Clean, 1 = Negative / Toxic
    """
    comment = str(comment_text).lower().strip()
    x = vect.transform([comment])
    pred = model.predict(x)[0]
    return 'NEGATIVE COMMENT' if pred == 1 else 'POSITIVE COMMENT'

print(predict_toxic('you are an idiot'

))

import pickle
import google.colab
Toxic_model = open('Toxic_model.pkl','wb')
pickle.dump(model, Toxic_model)
google.colab.files.download('Toxic_model.pkl')

df["comment_length"] = df["comment_text"].astype(str).apply(len)
plt.hist(df["comment_length"], bins=40)
plt.title("Distribution of Comment Lengths")
plt.xlabel("Length of Comment (characters)")
plt.ylabel("Frequency")
plt.show()

category_cols = ["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]
df[category_cols].sum().plot(kind="bar"),
plt.title("Count of Toxic Categories")
plt.ylabel("Number of Toxic Labels")
plt.show()

df["overall_toxic"] = df[category_cols].max(axis=1) # 1 if any toxic category

toxic_count = df["overall_toxic"].value_counts()

plt.pie(toxic_count, labels=["Non-Toxic", "Toxic"], autopct="%1.1f%%", startangle=90)
plt.title("Overall Toxic vs Non-Toxic Distribution")
plt.legend()
plt.show()